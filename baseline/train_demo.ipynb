{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a3ad885",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/19 09:52:21 INFO mlflow.tracking.fluent: Experiment with name 'demo_experiment' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "import torch\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import os\n",
    "# connect the experiment to the tracking server\n",
    "mlflow.set_tracking_uri(\"http://0.0.0.0:5001\")\n",
    "\n",
    "# set the experiment name\n",
    "mlflow.set_experiment(\"demo_experiment\")\n",
    "\n",
    "# enable autologging\n",
    "mlflow.pytorch.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15187e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging to a file\n",
    "import logging\n",
    "\n",
    "# Get current working directory\n",
    "log_file_path = os.path.join(os.getcwd(), 'training.log')\n",
    "\n",
    "# Set up the logger manually\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Remove all previous handlers (important in Jupyter)\n",
    "logger.handlers.clear()\n",
    "\n",
    "# Create and add FileHandler\n",
    "file_handler = logging.FileHandler(log_file_path, mode='w')\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Optional: also log to console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fd6b7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 09:52:25,345 - INFO - MPS backend is available. It uses the global PyTorch seed.\n",
      "2025-08-19 09:52:25,346 - INFO - Deterministic algorithms set to True.\n",
      "2025-08-19 09:52:25,346 - INFO - Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "from utils.seed import set_seed\n",
    "# Set a global seed for reproducibility\n",
    "global_seed = 42\n",
    "set_seed(global_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25449091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 09:52:25,434 - INFO - Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNetV3SmallWithCustomHead(\n",
       "  (head): CNNPatchDownscaleHead(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(32, 3, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "    )\n",
       "  )\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): Conv2dNormActivation(\n",
       "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "  )\n",
       "  (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=576, out_features=128, bias=True)\n",
       "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "    (5): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Dropout(p=0.5, inplace=False)\n",
       "    (9): Linear(in_features=64, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "from models.MobileNet import MobileNetV3SmallWithCustomHead\n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 5  # change according to your dataset\n",
    "dropout = 0.5  # Set dropout rate\n",
    "# learning rates\n",
    "base_lr = 0.001          # head + classifier\n",
    "encoder_lr = base_lr*0.1  # encoder when unfrozen\n",
    "\n",
    "model = MobileNetV3SmallWithCustomHead(num_classes, dropout)\n",
    "\n",
    "# Check device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.backends.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "logging.info(f\"Using device: {device.type}\")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38dddc4",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "777d8920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from utils.regularization import EarlyStopping\n",
    "from utils.train_validate import train, validate\n",
    "from utils.transfer_learning import freeze_encoder, unfreeze_last_layers, get_optimizer\n",
    "\n",
    "# batch size\n",
    "dataloader_eval_batch = 8\n",
    "dataloader_train_batch = 8\n",
    "\n",
    "# Initialize optimizer\n",
    "starting_lr = base_lr\n",
    "optimizer = optimizer = get_optimizer(\n",
    "    model,\n",
    "    base_lr=starting_lr, \n",
    "    encoder_lr=0.0001, \n",
    "    weight_decay=0.0001\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='max',           # maximize validation accuracy or F1\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    cooldown=2,\n",
    "    min_lr=1e-6,\n",
    "    threshold=0.0001,\n",
    "    threshold_mode='abs',\n",
    "    # verbose=True\n",
    ")\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping_patience = scheduler.cooldown + scheduler.patience + 5\n",
    "early_stopping_delta = scheduler.threshold / 2\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=early_stopping_patience,\n",
    "    verbose=True,\n",
    "    min_delta=early_stopping_delta\n",
    ")\n",
    "\n",
    "unfreeze_encoder_at = 20\n",
    "max_num_epochs = 300\n",
    "\n",
    "\n",
    "\n",
    "# Log params to MLflow\n",
    "params = {\n",
    "    \"global_seed\": global_seed,\n",
    "    \"train_batch_size\": dataloader_train_batch,\n",
    "    \"eval_batch_size\": dataloader_eval_batch,\n",
    "    \"loss_function\": \"CrossEntropyLoss\",\n",
    "    \"learning_rate\": starting_lr,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"scheduler_mode\": \"max\",\n",
    "    \"scheduler_factor\": 0.5,\n",
    "    \"scheduler_patience\": 2,\n",
    "    \"scheduler_cooldown\": 0,\n",
    "    \"scheduler_min_lr\": 1e-6,\n",
    "    \"scheduler_threshold\": 0.0001,\n",
    "    \"scheduler_threshold_mode\": \"abs\",\n",
    "    \"scheduler\": \"ReduceLROnPlateau\",\n",
    "    \"early_stopping_patience\": early_stopping_patience,\n",
    "    \"early_stopping_delta\": early_stopping_delta,\n",
    "    \"max_epochs\": max_num_epochs,\n",
    "    \"device\": device.type,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"dropout\": dropout\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe694a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 09:53:20,071 - INFO - Train class counts: [200, 200, 600, 8000, 400]\n",
      "2025-08-19 09:53:20,072 - INFO - Val   class counts: [200, 200, 400, 2000, 200]\n",
      "2025-08-19 09:53:20,074 - INFO - Train class weights: [0.004999999888241291, 0.004999999888241291, 0.0016666667070239782, 0.0001250000059371814, 0.0024999999441206455]\n",
      "2025-08-19 09:53:20,075 - INFO - Val   class weights: [0.004999999888241291, 0.004999999888241291, 0.0024999999441206455, 0.0005000000237487257, 0.004999999888241291]\n",
      "2025-08-19 09:53:20,320 - INFO - Started MLflow run with ID: d4f05dcd0d9a420792247e0da8c0ca89\n",
      "2025-08-19 09:53:22,458 - INFO - Model report logged to MLflow\n",
      "2025-08-19 09:53:22,463 - INFO - Starting training...\n",
      "2025-08-19 09:53:22,464 - INFO -  Encoder frozen (training head + classifier only)\n",
      "2025-08-19 09:53:22,475 - INFO - Epoch 1, Head LR: 0.001, Encoder LR: 0.0001\n",
      "2025-08-19 10:00:11,030 - INFO - Epoch 1 - Train Loss: 0.7963, Accuracy: 0.5091, F1(weighted): 0.4368, F1(macro): 0.4386\n",
      "2025-08-19 10:01:25,526 - INFO - Epoch 1 - Val Loss: 2.2788, Accuracy: 0.1770, F1(weighted): 0.0892, F1(macro): 0.2390\n",
      "\u001b[31m2025/08/19 10:01:31 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "2025-08-19 10:01:31,193 - INFO - Improved F1 (weighted): 0.089203. Saved best model.\n",
      "\u001b[31m2025/08/19 10:01:34 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "2025-08-19 10:01:34,732 - INFO - Monitored metric improved to 0.089203\n",
      "2025-08-19 10:01:34,738 - INFO - Epoch 2, Head LR: 0.001, Encoder LR: 0.0001\n",
      "                                                                               "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "from utils.datasets import AugmentedDataset\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "def main(num_workers=4):\n",
    "    input_transform = transforms.Compose([\n",
    "        transforms.Resize((720, 1270)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "    ])\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(42)\n",
    "\n",
    "    # Load datasets\n",
    "    full_dataset = AugmentedDataset(\"augmented_data_no_pad/train\", transform=input_transform)\n",
    "    test_dataset = AugmentedDataset(\"augmented_data_no_pad/test\", transform=input_transform)\n",
    "\n",
    "\n",
    "    # ===== NEW: Extract labels directly =====\n",
    "    if hasattr(full_dataset, \"labels\"):\n",
    "        train_targets = np.array(full_dataset.labels)\n",
    "    elif hasattr(full_dataset, \"targets\"):\n",
    "        train_targets = np.array(full_dataset.targets)\n",
    "    else:\n",
    "        raise AttributeError(\"Your AugmentedDataset must store labels in .labels or .targets\")\n",
    "\n",
    "    if hasattr(test_dataset, \"labels\"):\n",
    "        val_targets = np.array(test_dataset.labels)\n",
    "    elif hasattr(test_dataset, \"targets\"):\n",
    "        val_targets = np.array(test_dataset.targets)\n",
    "    else:\n",
    "        raise AttributeError(\"Your AugmentedDataset must store labels in .labels or .targets\")\n",
    "\n",
    "    num_classes = len(np.unique(train_targets))\n",
    "\n",
    "    # ===== Compute class weights separately for train and val =====\n",
    "    train_class_counts = np.bincount(train_targets, minlength=num_classes)\n",
    "    val_class_counts   = np.bincount(val_targets, minlength=num_classes)\n",
    "\n",
    "    train_class_weights = torch.tensor(1.0 / (train_class_counts + 1e-6), dtype=torch.float32, device=device)\n",
    "    val_class_weights   = torch.tensor(1.0 / (val_class_counts + 1e-6), dtype=torch.float32, device=device)\n",
    "\n",
    "    logging.info(f\"Train class counts: {train_class_counts.tolist()}\")\n",
    "    logging.info(f\"Val   class counts: {val_class_counts.tolist()}\")\n",
    "    logging.info(f\"Train class weights: {train_class_weights.tolist()}\")\n",
    "    logging.info(f\"Val   class weights: {val_class_weights.tolist()}\")\n",
    "\n",
    "    # ===== Class-balanced CrossEntropyLoss for train and val =====\n",
    "    train_criterion = nn.CrossEntropyLoss(weight=train_class_weights)\n",
    "    val_criterion   = nn.CrossEntropyLoss(weight=val_class_weights)\n",
    "\n",
    "    # Compute sample weights for WeightedRandomSampler (train only)\n",
    "    train_sample_weights = train_class_weights[train_targets]\n",
    "\n",
    "    # Compute sample weights for WeightedRandomSampler (val only)\n",
    "    val_sample_weights = val_class_weights[val_targets]\n",
    "\n",
    "    # IMPORTANT: move to CPU + float32 (MPS cannot handle float64)\n",
    "    train_sampler = WeightedRandomSampler(\n",
    "        weights     = train_sample_weights.cpu().to(torch.float32),\n",
    "        num_samples = len(train_sample_weights),\n",
    "        replacement = True\n",
    "    )\n",
    "\n",
    "    # Create the sampler for the validation set\n",
    "    val_sampler = WeightedRandomSampler(\n",
    "        weights     = val_sample_weights.cpu().to(torch.float32),\n",
    "        num_samples = len(val_sample_weights),\n",
    "        replacement = True\n",
    "    )\n",
    "    \n",
    "            # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        full_dataset,\n",
    "        batch_size=dataloader_train_batch,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=dataloader_eval_batch,\n",
    "        shuffle  = False,  # no need to shuffle validation data\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    # ===== TRAINING LOOP =====\n",
    "    with mlflow.start_run() as run:\n",
    "        run_id = run.info.run_id\n",
    "        logging.info(f\"Started MLflow run with ID: {run_id}\")\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        try:\n",
    "            model_summary = summary(model, input_size=(1, 3, 720, 1270), device=device.type)\n",
    "\n",
    "            with open(\"model_report.txt\", \"w\") as f:\n",
    "                f.write(\"### MODEL ARCHITECTURE ###\\n\")\n",
    "                f.write(str(model))\n",
    "                f.write(\"\\n\\n### MODEL SUMMARY ###\\n\")\n",
    "                f.write(str(model_summary))\n",
    "\n",
    "            mlflow.log_artifact(\"model_report.txt\")\n",
    "            logging.info(\"Model report logged to MLflow\")\n",
    "            os.remove(\"model_report.txt\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to log model report: {e}\")\n",
    "            raise\n",
    "    \n",
    "        logging.info(\"Starting training...\")\n",
    "\n",
    "        best_f1 = 0.0\n",
    "        best_save_epoch = 0\n",
    "        train_dir = \"augmented_data_no_pad/train\"\n",
    "        class_names = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])\n",
    "\n",
    "        for epoch in range(max_num_epochs):\n",
    "            # --- Freezing / unfreezing logic ---\n",
    "            if epoch == 0:\n",
    "                freeze_encoder(model)\n",
    "\n",
    "            if epoch == unfreeze_encoder_at:  # after 20 epochs, unfreeze last few encoder layers\n",
    "                unfreeze_last_layers(model, num_blocks=2)\n",
    "                # lower LR for encoder param group\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    if param_group[\"lr\"] < base_lr:  # encoder group\n",
    "                        param_group[\"lr\"] = base_lr * 0.1\n",
    "                logging.info(\"Adjusted LR for encoder fine-tuning\")\n",
    "            \n",
    "            head_lr    = optimizer.param_groups[0]['lr']\n",
    "            encoder_lr = optimizer.param_groups[1]['lr']\n",
    "\n",
    "            mlflow.log_metric(\"head_lr\", head_lr, step=epoch + 1)\n",
    "            mlflow.log_metric(\"encoder_lr\", encoder_lr, step=epoch + 1)\n",
    "\n",
    "            logging.info(f\"Epoch {epoch+1}, Head LR: {head_lr}, Encoder LR: {encoder_lr}\")\n",
    "\n",
    "\n",
    "            train(model, train_loader, train_criterion, optimizer, device, epoch, class_names)\n",
    "            val_f1 = validate(model, val_loader, val_criterion, device, epoch, class_names)\n",
    "\n",
    "            scheduler.step(val_f1)\n",
    "\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                best_save_epoch = epoch\n",
    "                mlflow.pytorch.log_model(model, artifact_path=\"best_model\")\n",
    "                logging.info(f\"Improved F1 (weighted): {val_f1:.6f}. Saved best model.\")\n",
    "\n",
    "            mlflow.pytorch.log_model(model, artifact_path=\"latest_model\")\n",
    "\n",
    "            early_stopping(val_f1, model)\n",
    "            if early_stopping.early_stop:\n",
    "                mlflow.log_param(\"best_model_saved_at\", best_save_epoch)\n",
    "                mlflow.log_param(\"early_stopping_triggered_at\", epoch)\n",
    "                logging.warning(\"Early stopping triggered. Stopping training.\")\n",
    "                model.load_state_dict(early_stopping.best_model_state)\n",
    "                break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(num_workers=4)  # safer for Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d12c631",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
